q()
=360*4,5
360*4,5
360*4.5
600*4,5
600*4.5
700*4.5
fx <- function(x) {
if(x < 1) {res <- 0}
else if (x>=1 && x<2) {res <-(1/3)*x -1/3}
else if (x>=2 && x<3) {res <- 1/3}
else if (x>=3 && x<5) {res <- 0}
else if (x>=5 && x<6) {res <- 1/3}
else if (x>=6 && x<7) {res <- (-1/3)*x + 7/3}
else if (x>=7) {res <-0}
return(res);
}
fx <- Vectorize(fx)
fx
fx <- function(x) {
if(x < 1) {res <- 0}
else if (x>=1 && x<2) {y <-(1/3)*x -1/3}
else if (x>=2 && x<3) {y <- 1/3}
else if (x>=3 && x<5) {y <- 0}
else if (x>=5 && x<6) {y <- 1/3}
else if (x>=6 && x<7) {y <- (-1/3)*x + 7/3}
else if (x>=7) {y <-0}
return(y);
}
View(fx)
View(fx)
fx(2)
fx(2:5)
wynik = ()
wynik = c()
for ( i in 2:5){
wynik = fx(i)
}
wynik
wynik = c()
for ( i in 2:5){
wynik[i] = fx(i)
}
fx <- Vectorize(fx)
fx <- function(x) {
if(x <= 1) {res <- 0}
else if (x>1 && x<=2) {res <-(1/3)*x -1/3}
else if (x>2 && x<=3) {res <- 1/3}
else if (x>3 && x<=5) {res <- 0}
else if (x>5 && x<=6) {res <- 1/3}
else if (x>6 && x<=7) {res <- (-1/3)*x + 7/3}
else if (x>7) {res <-0}
return(res);
}
fx <- Vectorize(fx) #### po co ta wektoryzacja
dx <- 0.01
x <- seq(0,10,by=dx)
xx <- seq(0,10,by=0.01)
F <- cumsum(fx(x)*dx)
plot(x,F,type="l", ylab= "Prawdopodobienstwo", main="Dystrybuanta")
F_odwrotna <- approxfun(F,x)
F_odwrotna <- approxfun(F,x)
i<-1:1000
W<- array ( runif ( i, min = 0, max = 1 ), 1000)
X<- apply ( W , 1 , FUN = F_odwrotna )
hist(X, ylab="Liczba zliczen")
plot(ecdf(X),ylab="Prawdopodobienstwo",main="Dystrybuanta empiryczna")
plot(x,fx,type="l", ylab= "Prawdopodobienstwo", main="Dystrybuanta")
plot(1:7,fx,type="l", ylab= "Prawdopodobienstwo", main="Dystrybuanta")
q()
q()
q()
#Zad 1a)
fx <- function(x) {
if(x <= 1) {res <- 0}
else if (x>1 && x<=2) {res <-(1/3)*x -1/3}
else if (x>2 && x<=3) {res <- 1/3}
else if (x>3 && x<=5) {res <- 0}
else if (x>5 && x<=6) {res <- 1/3}
else if (x>6 && x<=7) {res <- (-1/3)*x + 7/3}
else if (x>7) {res <-0}
return(res);
}
str(fx)
fx <- Vectorize(fx)
str(fx)
cumsum(1:10)
cumsum(1)
cumsum(1:2)
dx <- 0.01
x <- seq(0,10,by=dx)
?seq
fx(x)*dx
F
F <- cumsum(fx(x)*dx) ### całeczka
F
plot(x,F,type="l", ylab= "Prawdopodobienstwo", main="Dystrybuanta")
F_odwrotna <- approxfun(F,x)
?approxfun
x <- c(2,2:4,4,4,5,5,7,7,7)
y <- c(1:6, 5:4, 3:1)
(amy <- approx(x, y, xout = x)$y) # warning, can be avoided by specifying 'ties=':
y
x
plot(x,y)
op <- options(warn=2) # warnings would be error
stopifnot(identical(amy, approx(x, y, xout = x, ties=mean)$y))
options(op) # revert
(ay  <- approx(x, y, xout = x, ties = "ordered")$y)
stopifnot(amy == c(1.5,1.5, 3, 5,5,5, 4.5,4.5, 2,2,2),
ay  == c(2, 2,    3, 6,6,6, 4, 4,    1,1,1))
approx(x, y, xout = x, ties = min)$y
approx(x, y, xout = x, ties = max)$y
approxfun(F,x)
F_odwrotna
F_odwrotna <- approxfun(F,x) ### sproksymuje funkcje
i<-1:1000
W<- array ( runif ( i, min = 0, max = 1 ), 1000)
W
?runif
array ( runif ( i, min = 0, max = 1 ), 1000)
array ( runif ( i, min = 0, max = 3 ), 1000)
plot(array ( runif ( i, min = 0, max = 3 ), 1000))
plot(array ( runif ( i, min = 0, max = 1 ), 1000))
X<- apply ( W , 1 , FUN = F_odwrotna )
X
hist(X, ylab="Liczba zliczen")
?apply
hist(apply ( W , 1 , FUN = F ), ylab="Liczba zliczen")
pomo=apply ( W , 1 , FUN = F )
F_odwrotna
F
apply
X<- apply ( W , 1 , FUN = fx )
hist(X, ylab="Liczba zliczen")
X<- apply ( W , 1 , FUN = F_odwrotna )
hist(X, ylab="Liczba zliczen")
F
cumsum(fx(x)*dx)
plot(x,F,type="l", ylab= "Prawdopodobienstwo", main="Dystrybuanta")
dx <- 0.01
x <- seq(0,10,by=dx)
F <- cumsum(fx(x)*dx) ### całeczka
plot(x,F,type="l", ylab= "Prawdopodobienstwo", main="Dystrybuanta")
?approxfun
F_odwrotna <- approxfun(F,x) ### interpolacja
i<-1:1000
W<- array ( runif ( i, min = 0, max = 1 ), 1000) ### generacja punktów pomiedzy 0 a 1
X<- apply ( W , 1 , FUN = F_odwrotna )
hist(X, ylab="Liczba zliczen")
plot(ecdf(X),ylab="Prawdopodobienstwo",main="Dystrybuanta empiryczna")
?ecdf
?ecdf
?frequency
q()
q()
q()
q()
x = c(-4.06, -4.89, -1, -2.87)
x^2
sum(x^2)
sum(x^2)/4
a = c( 0.04,  0.87,  0.52,  0)
b = c(0.8 ,  0.43,  0.06,  0.44)
c= c(0.41,  0.03,  0.89,  0.24)
mutant = a + 0.8*(b-c)
mutant_norm = norm(mutant)
mutant_norm = norm(mutant, p =1)
?norm
mutant_norm = norm(mutant, p =1)
q()
q()
q()
q()
q()
q()
q()
q()
q()
q()
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
dimnames(x)[[1]] <- letters[1:8]
apply(x, 2, mean, trim = .2)
col.sums <- apply(x, 2, sum)
View(x)
View(x)
summary(apply(x, 2, mean, trim = .2))
apply(x, 2, median)
summary(apply(x, 2, median))
q()
print("hello")
q()
- (0.6*log(0.6, base = 10)+0.4*log(0.4, base = 10))
- (0.3*log(0.3, base = 10)+0.4*log(0.4, base = 10)+0.3*log(0.3, base = 10))
- (0.5*log(0.5, base = 10)+0.5*log(0.5, base = 10))
- ((2/5)*log(2/5, base = 10)+(3/5)*log(3/5, base = 10))
- ((4/5)*log(4/5, base = 10)+(1/5)*log(1/5, base = 10))
log(0,base=10)
- (0.6*log(0.6, base = 10)+0.4*log(0.4, base = 10))
/1/6
1/6
- ((8/10))*log(8/10, base = 10)+(0.2)*log(0.2, base = 10))
- ((-.8))*log(-.8, base = 10)+(0.2)*log(0.2, base = 10))
- (0.8))*log(0.8, base = 10)+(0.2)*log(0.2, base = 10))
- (0.8)*log(0.8, base = 10)+(0.2)*log(0.2, base = 10))
- (0.8*log(0.8, base = 10)+(0.2)*log(0.2, base = 10))
- (7/10*log(7/10, base = 10)+(3/10)*log(3/10, base = 10))
q()
q()
if (! "randomForest" %in% row.names(installed.packages()))
install.packages("randomForest")
err <- function(y.true, y.pred) { sum(y.pred!=y.true)/length(y.true) }
predc <- function(...) { predict(..., type="c") }
library(e1071)
library(randomForest)
library(tidyverse)
library(tidyr)
library(caret)
students<- read.csv("../data/students.csv",header=TRUE)
students$X <- NULL
students$Walc = as.factor(students$Walc)
students$Dalc = as.factor(students$Dalc)
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
setwd("~/MOW_2/MOW-students-alcohol-consumption/src")
students<- read.csv("../data/students.csv",header=TRUE)
students$X <- NULL
students$Walc = as.factor(students$Walc)
students$Dalc = as.factor(students$Dalc)
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
seed <- 7
metric <- "Accuracy"
set.seed(seed)
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree", "maxnodes", "replace"), class = c(rep("numeric", 3), "logical"), label = c("mtry", "ntree", "maxnodes", "replace"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
randomForest(x, y, mtry = param$mtry, ntree=param$ntree, maxnodes=param$maxnodes, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
# train model
control <- trainControl(## 10-fold CV
method = "cv",
number = 10)
#control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(mtry=c(1:10), ntree=c(10,50,100,300,500), maxnodes=c(3,10,20,50), replace=c(TRUE, FALSE))
set.seed(seed)
custom_D <- train(Dalc~., data=train, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
summary(custom_D)
custom_D$bestTune
#png("custom_D.png")
plot(custom_D)
custom_D$results
custom_D$bestTune
custom_D$[248]
custom_D$results[248]
custom_D$finalModel
if (! "randomForest" %in% row.names(installed.packages()))
install.packages("randomForest")
err <- function(y.true, y.pred) { sum(y.pred!=y.true)/length(y.true) }
predc <- function(...) { predict(..., type="c") }
library(e1071)
library(randomForest)
library(tidyverse)
library(tidyr)
library(caret)
students<- read.csv("../data/students.csv",header=TRUE)
students$X <- NULL
students$Walc = as.factor(students$Walc)
students$Dalc = as.factor(students$Dalc)
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
##dobór najlepszego modelu
##Mona dobrać dzięki temu 3 parametry. Uwaga, dosyć długo się liczy.
##sugerowałabym mtry=9-11, maxnodes >=50), ntrees = 100-300, dla 500 liczy się lepiej, ale wydłuża proces, a charekterystykę widać już przy 300.
seed <- 7
metric <- "Accuracy"
set.seed(seed)
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree", "maxnodes", "replace"), class = c(rep("numeric", 3), "logical"), label = c("mtry", "ntree", "maxnodes", "replace"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
randomForest(x, y, mtry = param$mtry, ntree=param$ntree, maxnodes=param$maxnodes, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
control <- trainControl(## 10-fold CV
method = "cv",
number = 10)
#control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegr
#control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(mtry=c(1:10), ntree=c(10,50,100,300,500), maxnodes=c(3,10,20,50), replace=c(TRUE, FALSE))
set.seed(seed)
custom_W <- train(Walc~., data=train, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
summary(custom_W)
custom_W$bestTune
plot(custom_W)
custom_W$finalModel
print(custom_W$finalModel)
RF_Walc = randomForest(Walc~., data=train, classwt = c(1, 2,3,4,5) , replace = FALSE, mtry=9, ntree= 50, maxnodes = 50)
prediction <-predict(RF_walc, test)
prediction <-predict(RF_Walc, test)
confusionMatrix(prediction, test$Walc)
RF_Walc = randomForest(Walc~., data=train, classwt = c(0.1,0.2,0.2,0.2,0.3) , replace = FALSE, mtry=9, ntree= 50, maxnodes = 50)
prediction <-predict(RF_Walc, test)
confusionMatrix(prediction, test$Walc)
cwt = c(0.1,0.2,0.2,0.2,0.3)
RF_Walc = randomForest(Walc~., data=train, classwt = cwt , replace = FALSE, mtry=9, ntree= 50, maxnodes = 50)
prediction <-predict(RF_Walc, test)
confusionMatrix(prediction, test$Walc)
cwt = c(1,2,3,4,5)
RF_Walc = randomForest(Walc~., data=train, classwt = cwt , replace = FALSE, mtry=9, ntree= 50, maxnodes = 50)
prediction <-predict(RF_Walc, test)
confusionMatrix(prediction, test$Walc)
cwt = c(1,2,2,2,2)
RF_Walc = randomForest(Walc~., data=train, classwt = cwt , replace = FALSE, mtry=9, ntree= 50, maxnodes = 50)
prediction <-predict(RF_Walc, test)
confusionMatrix(prediction, test$Walc)
cwt = c(1,5,5,5,5)
RF_Walc = randomForest(Walc~., data=train, classwt = cwt , replace = FALSE, mtry=9, ntree= 50, maxnodes = 50)
prediction <-predict(RF_Walc, test)
confusionMatrix(prediction, test$Walc)
cwt = c(0.5, 2,2,2,2)
RF_Walc = randomForest(Walc~., data=train, classwt = cwt , replace = FALSE, mtry=9, ntree= 50, maxnodes = 50)
prediction <-predict(RF_Walc, test)
confusionMatrix(prediction, test$Walc)
# 5  0  1  0  1  6
#
# Overall Statistics
#
# Accuracy : 0.3158
# 95% CI : (0.238, 0.402)
# No Information Rate : 0.4586
# P-Value [Acc > NIR] : 0.9997
#
# Kappa : 0.1879
cwt = c(0.1, 2,2,2,2)
RF_Walc = randomForest(Walc~., data=train, classwt = cwt , replace = FALSE, mtry=9, ntree= 50, maxnodes = 50)
prediction <-predict(RF_Walc, test)
confusionMatrix(prediction, test$Walc)
