cls = c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
library(tidyr)
library(ggplot2)
library(Hmisc)
mat_students <- read.csv("student-mat.csv",header=TRUE)
por_students <- read.csv("student-por.csv",header=TRUE)
students = rbind(mat_students, por_students)
cls = c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
students <- students[!duplicated(students[,cls]),]
library(tidyr)
library(ggplot2)
library(Hmisc)
mat_students <- read.csv("../data/student-mat.csv",header=TRUE)
por_students <- read.csv("../data/student-por.csv",header=TRUE)
students = rbind(mat_students, por_students)
cls = c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
students <- students[!duplicated(students[,cls]),]
c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink")
hgD = hist(students$Dalc,plot = FALSE)
hgW =  hist(students$Walc, plot = FALSE)
hgD$density = hgD$counts/sum(hgD$counts)*100
hgW$density = hgW$counts/sum(hgW$counts)*100
plot(hgD, freq=FALSE, col = c2, main="Spożycie alkoholu", ylab = "Studenci [%]", xlab= "od 1 - bardzo rzadko to 5 - bardzo często") # Plot 1st histogram using a transparent color
plot(hgW,   freq=FALSE, col = c1, add = TRUE) # Add 2nd histogram using different color
legend("topright",
c("weekend", "dzień pracujący"),
lty=c(10, 2, 10),
col=c(c1,c2),
bty = "n")
summary(students)
qqnorm(students$Dalc)
hist(students$Dalc)
hist(students$Dalc, breaks=seq(0,5, 1))
hist(students$Dalc, breaks=seq(0,6, 1))
hist(students$Dalc, breaks=seq(1,6, 1))
hist(students$Dalc, breaks=seq(1,5, 1))
View(students)
View(students)
View(students)
hist(students$Dalc, breaks=seq(0,5, 1))
hist(students$Dalc, breaks=seq(0.5,5, 1))
hist(students$Dalc, breaks=seq(-1,5, 1))
write.csv(students,'students.csv')
rm(list=ls())
dev.off()
graphics.off()
students<- read.csv("../data/students.csv",header=TRUE)
View(students)
View(students)
#biblioteka do bayesa
library(e1071)
library(randomForest)
students<- read.csv("../data/students.csv",header=TRUE)
install.packages("randomForest")
library(e1071)
library(randomForest)
#biblioteka do bayesa
if (! "e1071" %in% row.names(installed.packages()))
install.packages("e1071")
if (! "randomForest" %in% row.names(installed.packages()))
install.packages("randomForest")
library(e1071)
library(randomForest)
students<- read.csv("../data/students.csv",header=TRUE)
students<- read.csv("../data/students.csv",header=TRUE)
sapply(students, class)
library(randomForest)
students<- read.csv("../data/students.csv",header=TRUE)
students <- students[~c(1)]
View(students)
View(students)
students$X <- NULL
View(students)
View(students)
sample = sample.split(students$num, SplitRatio = .8)
sample = students.split(students$num, SplitRatio = .8)
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
err <- function(y.true, y.pred) { sum(y.pred!=y.true)/length(y.true) }
predc <- function(...) { predict(..., type="c") }
library(e1071)
library(randomForest)
library(tidyverse)
library(tidyr)
library(caret)
students<- read.csv("../data/students.csv",header=TRUE)
students$X <- NULL
students$Walc = as.factor(students$Walc)
students$Dalc = as.factor(students$Dalc)
par.cost=2^(-4:4)
#control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(mtry=c(1:10), ntree=c(10,50,100,300,500), maxnodes=c(3,10,20,50))
set.seed(seed)
g.tuneC <- tune(randomForest, Dalc~ ., data=train, ranges=list(cost=par.cost),
tunecontrol=tune.control(nrepeat=10))
par.cost=(-4:4)
#control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(mtry=c(1:10), ntree=c(10,50,100,300,500), maxnodes=c(3,10,20,50))
set.seed(seed)
g.tuneC <- tune(randomForest, Dalc~ ., data=train, ranges=list(cost=par.cost),
tunecontrol=tune.control(nrepeat=10))
# błędy walidacji krzyżowej dla poszczególnych wartości
g.tuneC$performances[order(g.tuneC$performances$error),]
# błąd modelu dla najlepszej wartości C na zbiorze trenującym i testowym
err(g.train$Type, predict(g.tuneC$best.model, g.train))
par.cost=(1:4)
#control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(mtry=c(1:10), ntree=c(10,50,100,300,500), maxnodes=c(3,10,20,50))
set.seed(seed)
g.tuneC <- tune(randomForest, Dalc~ ., data=train, ranges=list(cost=par.cost),
tunecontrol=tune.control(nrepeat=10))
# błędy walidacji krzyżowej dla poszczególnych wartości
g.tuneC$performances[order(g.tuneC$performances$error),]
# błąd modelu dla najlepszej wartości C na zbiorze trenującym i testowym
err(g.train$Type, predict(g.tuneC$best.model, g.train))
err(g.test$Type, predict(g.tuneC$best.model, g.test))
tuneC <- tune(randomForest, Dalc~ ., data=train, ranges=list(cost=par.cost),
tunecontrol=tune.control(nrepeat=10))
tuneC <- tune(randomForest, Dalc~ ., data=train, mtry=list(cost=par.cost), maxnodes = 60, ntree= 300,
tunecontrol=tune.control(nrepeat=10))
tuneC <- tune(randomForest, Dalc~ ., data=train, mtry=list(par.cost), maxnodes = 60, ntree= 300,
tunecontrol=tune.control(nrepeat=10))
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
par.cost=(1:4)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(mtry=c(1:10), ntree=c(10,50,100,300,500), maxnodes=c(3,10,20,50))
set.seed(seed)
tuneC <- tune(randomForest, Dalc~ ., data=train, mtry=list(par.cost), maxnodes = 60, ntree= 300,
tunecontrol=control)
students$Walc = as.factor(students$Walc)
students$Dalc = as.factor(students$Dalc)
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
mtryAr = c(1:30)
View(students)
# For classification
data(iris)
iris$Species <- as.factor(iris$Species)
set.seed(1234)
( rf.mdl <- randomForest(iris[,1:4], iris[,"Species"], ntree=501) )
( rf.cv <- rf.crossValidation(rf.mdl, iris[,1:4], p=0.10, n=99, ntree=501) )
# Plot cross validation versus model producers accuracy
par(mfrow=c(1,2))
library(rfUtilities)
install.packages("rfUtilities")
library(rfUtilities)
# For classification
data(iris)
iris$Species <- as.factor(iris$Species)
set.seed(1234)
( rf.mdl <- randomForest(iris[,1:4], iris[,"Species"], ntree=501) )
( rf.cv <- rf.crossValidation(rf.mdl, iris[,1:4], p=0.10, n=99, ntree=501) )
# Plot cross validation versus model producers accuracy
par(mfrow=c(1,2))
plot(rf.cv, type = "cv", main = "CV producers accuracy")
plot(rf.cv, type = "model", main = "Model producers accuracy")
# Plot cross validation versus model oob
par(mfrow=c(1,2))
plot(rf.cv, type = "cv", stat = "oob", main = "CV oob error")
plot(rf.cv, type = "model", stat = "oob", main = "Model oob error")
mtryDf<- data.frame(mtry=integer(),
err=double(),
errTest=double())
maxNodesDf<- data.frame(maxNodes=integer(),
err=double(),
errTest=double())
numTreesDf <- data.frame(numtrees=integer(),
err=double(),
errTest=double())
mtryAr = c(1:30)
numTreesAr = c(1, 10, 30, 50, 100, 300, 500, 1000, 3000)
maxNodseAr = c(1, 5, 10, 25, 30, 60, 100)
iters = 20
constmMtry = 9
constMaxNodes = 60
constNtrees = 300
for (att in mtryAr){
for(i in iters){
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
model <- randomForest(Dalc ~ . , data = train, mtry = att, ntree =constNtrees, replace = TRUE, maxnodes = constMaxNodes )
newValues <- data.frame(att, err(train$Dalc, predict(model, train)), err(test$Dalc, predict(model, test)) )
names(newValues) <- c("mtry", "err", "errTest")
mtryDf.Newdf <- rbind(DataFrame.mtryDf, newValues)
}
}
constNtrees = 300
for (att in mtryAr){
for(i in iters){
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
model <- randomForest(Dalc ~ . , data = train, mtry = att, ntree =constNtrees, replace = TRUE, maxnodes = constMaxNodes )
newValues <- data.frame(att, err(train$Dalc, predict(model, train)), err(test$Dalc, predict(model, test)) )
names(newValues) <- c("mtry", "err", "errTest")
mtryDf.Newdf <- rbind(mtryDf, newValues)
}
}
View(mtryDf)
View(mtryDf.Newdf)
for (att in mtryAr){
for(i in iters){
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
model <- randomForest(Dalc ~ . , data = train, mtry = att, ntree =constNtrees, replace = TRUE, maxnodes = constMaxNodes )
newValues <- data.frame(att, err(train$Dalc, predict(model, train)), err(test$Dalc, predict(model, test)) )
names(newValues) <- c("mtry", "err", "errTest")
mtryDf <- rbind(mtryDf, newValues)
}
}
View(mtryDf)
for (att in mtryAr){
for(i in 1:iters){
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
model <- randomForest(Dalc ~ . , data = train, mtry = att, ntree =constNtrees, replace = TRUE, maxnodes = constMaxNodes )
newValues <- data.frame(att, err(train$Dalc, predict(model, train)), err(test$Dalc, predict(model, test)) )
names(newValues) <- c("mtry", "err", "errTest")
mtryDf <- rbind(mtryDf, newValues)
}
}
boxplot(err~mtry,
data=mtryDf,
# main="Different boxplots for each month",
xlab="mytry",
ylab="błąd",
#  col="orange",
#  border="brown"
)
View(mtryDf)
View(mtryDf)
boxplot(err~mtry,
data=mtryDf,
xlab="mytry",
ylab="błąd",
)
boxplot(mtry~err,
data=mtryDf,
xlab="mytry",
ylab="błąd",
)
boxplot(mtry~err,
data=mtryDf,
xlab="mytry",
ylab="błąd",
)
install.packages("graphics")
install.packages("graphics")
install.packages("graphics")
install.packages("graphics")
err <- function(y.true, y.pred) { sum(y.pred!=y.true)/length(y.true) }
predc <- function(...) { predict(..., type="c") }
library(e1071)
library(randomForest)
library(tidyverse)
library(tidyr)
library(caret)
library(rfUtilities)
students<- read.csv("../data/students.csv",header=TRUE)
students$X <- NULL
students$Walc = as.factor(students$Walc)
mtryDf<- data.frame(mtry=integer(),
err=double(),
errTest=double())
maxNodesDf<- data.frame(maxNodes=integer(),
err=double(),
errTest=double())
numTreesDf <- data.frame(numtrees=integer(),
err=double(),
errTest=double())
mtryAr = c(1:30)
numTreesAr = c(1, 10, 30, 50, 100, 300, 500, 1000, 3000)
maxNodseAr = c(1, 5, 10, 25, 30, 60, 100)
iters = 20
constmMtry = 9
constMaxNodes = 60
constNtrees = 300
for (att in mtryAr){
for(i in 1:iters){
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
model <- randomForest(Dalc ~ . , data = train, mtry = att, ntree =constNtrees, replace = TRUE, maxnodes = constMaxNodes )
newValues <- data.frame(att, err(train$Dalc, predict(model, train)), err(test$Dalc, predict(model, test)) )
names(newValues) <- c("mtry", "err", "errTest")
mtryDf <- rbind(mtryDf, newValues)
}
}
boxplot(mtry~err,
data=mtryDf,
xlab="mytry",
ylab="błąd",
)
boxplot(mtry~err, data=mtryDf,xlab="mytry", ylab="błąd")
boxplot(err~mtry, data=mtryDf,xlab="mytry", ylab="błąd")
View(mtryDf)
rm(list=ls())
graphics.off()
err <- function(y.true, y.pred) { sum(y.pred!=y.true)/length(y.true) }
predc <- function(...) { predict(..., type="c") }
library(e1071)
predc <- function(...) { predict(..., type="c") }
library(e1071)
library(randomForest)
library(tidyverse)
library(tidyr)
library(caret)
library(rfUtilities)
students<- read.csv("../data/students.csv",header=TRUE)
students$X <- NULL
students$Walc = as.factor(students$Walc)
students$Dalc = as.factor(students$Dalc)
# For classification
data(iris)
iris$Species <- as.factor(iris$Species)
set.seed(1234)
( rf.mdl <- randomForest(iris[,1:4], iris[,"Species"], ntree=501) )
( rf.cv <- rf.crossValidation(rf.mdl, iris[,1:4], p=0.10, n=99, ntree=501) )
# Plot cross validation versus model producers accuracy
par(mfrow=c(1,2))
plot(rf.cv, type = "cv", main = "CV producers accuracy")
plot(rf.mdl, type = "model", main = "Model producers accuracy")
# Plot cross validation versus model oob
par(mfrow=c(1,2))
plot(rf.cv, type = "cv", stat = "oob", main = "CV oob error")
plot(rf.mdl, type = "model", stat = "oob", main = "Model oob error")
# Plot cross validation versus model producers accuracy
par(mfrow=c(1,2))
plot(rf.cv, type = "cv", main = "CV producers accuracy")
plot(rf.cv, type = "model", main = "Model producers accuracy")
# Plot cross validation versus model oob
par(mfrow=c(1,2))
plot(rf.cv, type = "cv", stat = "oob", main = "CV oob error")
plot(rf.cv, type = "model", stat = "oob", main = "Model oob error")
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
mtryDf<- data.frame(mtry=integer(),
err=double(),
errTest=double())
maxNodesDf<- data.frame(maxNodes=integer(),
err=double(),
errTest=double())
numTreesDf <- data.frame(numtrees=integer(),
err=double(),
errTest=double())
mtryAr = c(1:30)
numTreesAr = c(1, 10, 30, 50, 100, 300, 500, 1000, 3000)
maxNodseAr = c(1, 5, 10, 25, 30, 60, 100)
iters = 20
constmMtry = 9
constMaxNodes = 60
constNtrees = 300
for (att in mtryAr){
for(i in 1:iters){
sample <- sample.int(n = nrow(students), size = floor(.8*nrow(students)), replace = F)
train <- students[sample, ]
test  <- students[-sample, ]
model <- randomForest(Dalc ~ . , data = train, mtry = att, ntree =constNtrees, replace = TRUE, maxnodes = constMaxNodes )
newValues <- data.frame(att, err(train$Dalc, predict(model, train)), err(test$Dalc, predict(model, test)) )
names(newValues) <- c("mtry", "err", "errTest")
mtryDf <- rbind(mtryDf, newValues)
}
}
boxplot(err~mtry, data=mtryDf,xlab="mytry", ylab="błąd")
boxplot(errTest~mtry, data=mtryDf,xlab="mytry", ylab="błąd")
