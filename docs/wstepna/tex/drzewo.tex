Modele drzew mogą zostać wykorzystane do zagadnień klasyfikacyjnych jak i regresyjnych.
Rekurencyjna struktura drzewa składa się z:
\begin{itemize}
    \item węzłów, które reprezentują testy na wartościach atrybutu;
    \item gałęzi, oznaczających możliwe wyniki testu dla danego węzła;
    \item liści, czyli decyzji o klasyfikacji obiektu.
\end{itemize}
Proces predykcji polega na przejściu od pierwszego węzła do pewnego liścia, ścieżką wyznaczaną przez gałęzie odpowiadające wartościom atrybutów danego przykładu.

Budowa drzewa wiążę się z wieloma ważnymi decyzjami, m.in. : czy utworzyć węzeł czy liść,  jakiego podziału dokonać w węźle oraz którą klasę będzie reprezentował dany liść. Rozbudowa drzewa zazwyczaj zostaje wstrzymana wtedy, gdy osiągnie ono maksymalną (ustaloną wcześniej) głębokość, bądź gdy zbiór obiektów przeznaczonych do budowania modelu jest już pusty lub jednorodny. Wyznaczanie etykiety danego liścia odbywa się zazwyczaj metodą większościową, tzn.  dla danego zbioru obiektów wybierana jest klasa
decyzyjna, czyli najliczniej reprezentowana w tym zbiorze.

Spośród możliwych modeli, preferowane są drzewa o prostszej budowie, dlatego dąży się do jak najszybszego uzyskania jednorodnych klas. Oznacza to, że podział aktualnego zbioru przykładów ma dać jak największy przyrost informacji, uzyskać najmniejszą nieczystość
klas po podziale. Do wyboru najlepszego podziału można wykorzystać m.in. indeks Giniego $GI$ (1) bądź klasyczną, teorio-informacyjną entropię $E$ \cite{SuthaharanShan2015MLMa}. Wybierany jest podział $t$, który maksymalizuje redukcję nieczystości, czyli
różnicę nieczystości przed podziałem i po podziale \cite{MOW}.
\begin{equation}
 \Delta GI(c|t) = GI(c)-GI(c|t).      
 \end{equation}
 \begin{equation}
    \Delta E(c|t) = E(c)-E(c|t),
\end{equation}
Definiuję się to jako różnicę między bezwarunkową i warunkową entropią (lub
analogicznie dla indeksu Giniego).
Entropia bezwarunkowa opisywana jest wzorem:
\begin{equation}
    E(c) = \sum_{d\in C} - P(c=d)\log P(c=d)
\end{equation}
,gdzie $C$ oznacza zbiór możliwych klas.
a entropia warunkowa, w zależności od podziału $t$ dla pewnego zbioru trenującego $T$ i wszystkich wyników tego podziału $R$:
\begin{equation}
    E(c|t) = \sum_{r\in R} \frac{|T_{t=r}|}{|T|}E_{t=r}(c)
\end{equation}
Podobnie indeks Giniego definiuje się jako:
\begin{equation}
    GI(c) = 1- \sum_{d\in C}P(c=d)^2
\end{equation}
a warunkowy indeks Giniego:
\begin{equation}
      GI(c|t) = \sum_{r\in R} \frac{|T_{t=r}|}{|T|}GI_{t=r}(c).
\end{equation}

Niejednokrotnie budowa drzewa nie kończy procesu tworzenia modelu. Jak już zostało wspomniane, preferowane są prostsze drzewa. W celu ograniczenia ryzyka nadmiernego dopasowania stosuje się tzw. przycinanie drzewa, czyli zastąpienia wybranych węzłów przez liście już po zakończeniu budowania drzewa. Zastosowanie tego mechanizmu po wstępnym skończeniu budowy pozwala ocenić wpływ danych węzłów na jakość predykcji. Może się okazać, że poddrzewo wychodzące z danego węzła nie zmniejsza znacząco błędu rzeczywistego, a jedynie zwiększa poziom skomplikowania modelu. Wśród metod przycinania drzew znajduja się m.in. REP (Reduced Error Pruning), MEP (Minimum Error Pruning) oraz CCP (Cost-Complexity Pruning) \cite{MOW}.

Podsumowując drzewa decyzyjne są dość nieskomplikowanymi i intuicyjnymi dla człowieka modelami, które zazwyczaj charakteryzują się dobrą jakością predykcji. Niestety są podatne na obiążenia - nadmierne dopasowanie, chociaż można temu przeciwdziałać, co może być jednak dość złożonym i kosztownym procesem.
